# main.py
import argparse
import time
from utils.api_client import get_openai_model_fn
from utils.data_loader import load_dataset
from utils.logger import save_batch_results
from core.tracker import CostTracker
from pipeline_core import run_mpllm_pipeline

def main():
    # 1. è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description="MPLLM Nano Runner")
    parser.add_argument('--task', type=str, choices=['trivia', 'codenames', 'logic'], help="Task to run")
    parser.add_argument('--data', type=str, help="Path to .jsonl dataset")
    parser.add_argument('--limit', type=int, default=1, help="Number of items to test")
    parser.add_argument('--interactive', action='store_true', help="Run in interactive mode")
    args = parser.parse_args()

    # 2. è¨­å®šæ¶æ§‹èˆ‡æ¨¡å‹åƒæ•¸ (ç”¨æ–¼æª”åç”Ÿæˆ)
    ARCHITECTURE = "MPLLM"
    # é€™è£¡ä½ å¯ä»¥æ ¹æ“šå¯¦éš›ä½¿ç”¨çš„æ¨¡å‹ä¿®æ”¹ï¼Œä¾‹å¦‚ "GPT4o-Mix"
    MODEL_NAME = "GPT4o-Mix" 
    TEST_MODE = "Batch" if args.limit > 1 else "Single"

    # 3. åˆå§‹åŒ–æ¨¡å‹ (Dependency Injection)
    # é€™è£¡æ··åˆä½¿ç”¨äº† nano å’Œ miniï¼Œç›®å‰éƒ½æŒ‡å‘ gpt-4o-mini
    mini_model = get_openai_model_fn("gpt-4o-mini")
    nano_model = get_openai_model_fn("gpt-4o-mini")
    
    models = {'mini': mini_model, 'nano': nano_model}
    tracker = CostTracker()

    # æ”¶é›†æ‰€æœ‰çµæœçš„å®¹å™¨
    batch_results = []

    # 4. äº’å‹•æ¨¡å¼
    if args.interactive or (not args.task and not args.data):
        print("=== Interactive Mode ===")
        user_q = input("Question: ")
        item = {"topic": user_q, "questions": []}
        
        start_time = time.time()
        ans, trace = run_mpllm_pipeline('trivia', item, models, tracker)
        duration = time.time() - start_time
        
        print(f"Answer: {ans}")
        # äº’å‹•æ¨¡å¼é€šå¸¸ä¸å¯«å…¥æ­£å¼å ±è¡¨ï¼Œæˆ–å¯è¦–ç‚º Single æ¸¬è©¦
        return

    # 5. æ•¸æ“šé›†æ¨¡å¼
    print(f"=== {ARCHITECTURE} Runner: {args.task} | Mode: {TEST_MODE} ===")
    dataset = load_dataset(args.task, args.data)
    if not dataset:
        print("âŒ No data found.")
        return

    from utils.evaluator import evaluate_response  
    # 6. æ‰¹æ¬¡åŸ·è¡Œ
    total_score = 0.0
    
    for i, item in enumerate(dataset[:args.limit]):
        print(f"\nğŸš€ Processing Item {i+1}/{args.limit}...")
        
        # é–‹å§‹è¨ˆæ™‚
        start_time = time.time()
        
        # åŸ·è¡Œ Pipeline
        final_ans, trace = run_mpllm_pipeline(args.task, item, models, tracker)
        
        # çµæŸè¨ˆæ™‚
        duration = time.time() - start_time
        
        # === æ–°å¢ï¼šåŸ·è¡Œè©•åˆ† ===
        eval_result = evaluate_response(args.task, final_ans, item)
        score = eval_result.get('score', 0)
        total_score += score
        
        print(f"ğŸ¤– Answer: {str(final_ans)[:60]}...") 
        print(f"â±ï¸  Time: {duration:.2f}s | ğŸ† Score: {score:.2f} ({eval_result.get('details')})")
        
        current_stats = tracker.get_summary()
        
        result_entry = {
            "id": i + 1,
            "task": args.task,
            "input_summary": str(item)[:100].replace("\n", " "),
            "final_answer": str(final_ans)[:200].replace("\n", " "),
            "tokens": current_stats['total_tokens'],
            "cost": current_stats['cost_usd'],
            "time": duration,
            # æ–°å¢æ¬„ä½
            "score": score,
            "eval_details": eval_result.get('details')
        }
        batch_results.append(result_entry)

    # 7. è¼¸å‡ºç¸½è¡¨
    task_info = {
        "architecture": ARCHITECTURE,
        "model": MODEL_NAME,
        "mode": TEST_MODE
    }
    save_batch_results(batch_results, task_info)

    # 8. çµ‚ç«¯æ©Ÿç¸½çµ
    print("\n" + "="*50)
    avg_score = total_score / len(batch_results) if batch_results else 0
    print(f"âœ… Completed {len(batch_results)} tasks.")
    print(f"ğŸ† Average Score: {avg_score:.2%}") # é¡¯ç¤ºå¹³å‡æº–ç¢ºç‡
    print(f"ğŸ’° Total Accumulative Cost: ${tracker.get_summary()['cost_usd']:.6f}")

if __name__ == "__main__":
    main()