import argparse
import time
import json
import os
import sys
import random

import utils.nesting_manager

# === [Imports] Pipelines ===
from pipeline_core import run_mpllm_pipeline 
import pipeline_nesting

# === [Imports] Utils & Core ===
from utils.api_client import get_openai_model_fn
from utils.data_loader import load_dataset
from utils.logger import save_batch_results
from utils.evaluator import evaluate_response
from core.tracker import CostTracker

def main():
    # === 0. è¨­å®šé è¨­è·¯å¾‘ ===
    DATA_TRIVIA_PATH = "data/trivia_creative_writing/trivia_creative_writing_100_n_5.jsonl"
    DATA_CODENAMES_PATH = "data/codenames_collaborative/codenames_50.jsonl"
    DATA_LOGIC_PATH = "data/logic_grid_puzzle/logic_grid_puzzle_200.jsonl"

    # 1. è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description="MPLLM Nano Runner")
    parser.add_argument('--task', type=str, choices=['trivia', 'codenames', 'logic'], help="Task to run")
    parser.add_argument('--mode', type=str, choices=['base', 'nesting'], default='base', help="Select pipeline architecture")
    parser.add_argument('--data', type=str, help="Path to .jsonl dataset")
    parser.add_argument('--limit', type=int, default=1, help="Number of items to test")
    parser.add_argument('--interactive', action='store_true', help="Run in interactive chat mode")
    args = parser.parse_args()

    # ==========================================
    # 1.1 äº’å‹•å¼é¸å–® (è‹¥ç„¡åƒæ•¸è¼¸å…¥)
    # ==========================================
    if not args.task and not args.interactive:
        print("\n" + "="*45)
        print(" ğŸ¤– MPLLM Launcher Menu")
        print("="*45)
        
        # --- é¸æ“‡ä»»å‹™ ---
        print("[Step 1] è«‹é¸æ“‡ä»»å‹™:")
        print(" 1. ğŸ“ Trivia (Creative Writing)")
        print(" 2. ğŸ•µï¸  Codenames")
        print(" 3. ğŸ§© Logic Puzzle")
        print("-" * 45)
        print(" 4. ğŸ’¬ è‡ªç”±å°è©±æ¨¡å¼ (Chat Mode)")
        print("="*45)
        
        choice = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é … (1-4): ").strip()
        
        if choice == '1': args.task = 'trivia'
        elif choice == '2': args.task = 'codenames'
        elif choice == '3': args.task = 'logic'
        elif choice == '4': args.interactive = True
        else:
            print("âš ï¸ ç„¡æ•ˆé¸é …ï¼Œé è¨­åŸ·è¡Œ Trivia")
            args.task = 'trivia'

        # --- é¸æ“‡ Pipeline æ¨¡å¼ ---
        if not args.interactive:
            print("\n" + "-"*45)
            print("[Step 2] é¸æ“‡ Pipeline æ¶æ§‹:")
            print(" 1. ğŸ› ï¸  Baseline (Standard Pipeline)")
            print(" 2. ğŸ§ª Nesting  (Experimental Pipeline)")
            m_choice = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é … (1-2): ").strip()
            if m_choice == '2': 
                args.mode = 'nesting'
            else:
                args.mode = 'base'

        # --- é¸æ“‡ æ•¸é‡ ---
        if not args.interactive:
            print("\n" + "-"*45)
            print(f"[Step 3] é¸æ“‡ '{args.task}' çš„åŸ·è¡Œæ¨¡å¼:")
            print(" 1. ğŸ² Random Single (éš¨æ©ŸæŠ½ 1 é¡Œ)")
            print(" 2. ğŸ“š Sequential Batch (ä¾åºæ¸¬ N é¡Œ)")
            
            mode_choice = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é … (1-2): ").strip()
            
            if mode_choice == '2':
                try:
                    limit_input = input("   è¯·è¾“å…¥è¦åŸ·è¡Œçš„é¡Œæ•¸ (ä¾‹å¦‚ 5, 10): ").strip()
                    args.limit = int(limit_input) if limit_input else 5
                except ValueError:
                    args.limit = 5
                print(f"   ğŸ“š æ¨¡å¼: ä¾åºåŸ·è¡Œå‰ {args.limit} é¡Œ")
            else:
                args.limit = 1
                print("   ğŸ² æ¨¡å¼: éš¨æ©ŸæŠ½å– 1 é¡Œ")

            time.sleep(0.5)

    # === 1.2 è‡ªå‹•å¡«å…¥è³‡æ–™è·¯å¾‘ ===
    if args.task and not args.data:
        if args.task == 'trivia': args.data = DATA_TRIVIA_PATH
        elif args.task == 'codenames': args.data = DATA_CODENAMES_PATH
        elif args.task == 'logic': args.data = DATA_LOGIC_PATH
        print(f"ğŸ“‚ Auto-selected data: {args.data}")

    # 2. åˆå§‹åŒ–æ¨¡å‹èˆ‡åƒæ•¸
    ARCHITECTURE = "MPLLM"
    # [User Request] ä½¿ç”¨å‹•æ…‹æ¨¡å‹åç¨±
    MODEL_NAME = f"GPT5-{args.mode.upper()}" 
    # [User Request] æ¸¬è©¦æ¨¡å¼åˆ¤æ–·
    TEST_MODE = "Batch" if args.limit > 1 else "Single"

    mini_model = get_openai_model_fn("gpt-5-mini")
    nano_model = get_openai_model_fn("gpt-5-nano")
    models = {'mini': mini_model, 'nano': nano_model}
    tracker = CostTracker()
    batch_results = []

    # [User Request] æ±ºå®š Pipeline å‡½å¼
    if args.mode == 'nesting':
        print("\nğŸ§ª Switching to NESTING Pipeline...")
        pipeline_run_fn = pipeline_nesting.run_pipeline
    else:
        print("\nğŸ› ï¸ Using BASELINE Pipeline...")
        pipeline_run_fn = run_mpllm_pipeline

    # === æ¨¡å¼ A: è‡ªç”±å°è©±æ¨¡å¼ ===
    if args.interactive:
        print("\n=== ğŸ’¬ Interactive Chat Mode (No Scoring) ===")
        while True:
            try:
                user_q = input("\nUser Topic: ")
                if user_q.lower() in ['exit', 'quit']: break
                ans, _ = pipeline_run_fn('trivia', {"topic": user_q, "questions": []}, models, tracker)
                print(f"\nğŸ¤– MPLLM: {ans}\n")
            except KeyboardInterrupt: break
            except Exception: break
        return

    # === æ¨¡å¼ B: æ•¸æ“šé›†è©•æ¸¬æ¨¡å¼ ===
    print(f"=== ğŸš€ Running: {args.task} | Mode: {TEST_MODE} | Limit: {args.limit} ===")
    
    dataset = load_dataset(args.task, args.data)
    if not dataset:
        print(f"âŒ Error: Cannot load data from {args.data}")
        return

    # ==========================================
    # [æ ¸å¿ƒ] é¡Œç›®é¸æ“‡é‚è¼¯
    # ==========================================
    items_to_process = []
    if args.limit == 1:
        if len(dataset) > 0:
            selected = random.choice(dataset)
            original_idx = dataset.index(selected) + 1
            print(f"ğŸ² Randomly selected Item #{original_idx}")
            items_to_process = [selected]
    else:
        items_to_process = dataset[:args.limit]
        print(f"ğŸ“š Selected top {len(items_to_process)} items sequentially.")

    # ==========================================
    # åŸ·è¡Œè¿´åœˆ (å«é€²åº¦æ¢èˆ‡å³æ™‚å­˜æª”)
    # ==========================================
    total_score = 0.0
    total_items = len(items_to_process)
    bar_length = 30
    
    # ç”¨æ–¼å­˜æª”çš„ Meta Info
    meta_info = {
        "architecture": ARCHITECTURE,
        "model": MODEL_NAME,
        "mode": TEST_MODE
    }

    print("\nProcessing...")

    for i, item in enumerate(items_to_process):
        start_time = time.time()
        
        try:
            # åŸ·è¡Œ Pipeline
            final_ans, trace = pipeline_run_fn(args.task, item, models, tracker)
            duration = time.time() - start_time
            
            # è©•åˆ†
            eval_result = evaluate_response(args.task, final_ans, item)
            score = eval_result.get('score', 0.0)
            details = eval_result.get('details', "")
            total_score += score
            
            # ç²å– Token
            run_tokens = trace.get('total_tokens', 0)
            
            # æ ¼å¼åŒ–è¼¸å‡º
            ans_str = json.dumps(final_ans, ensure_ascii=False)
            
            # è¨˜éŒ„
            current_stats = tracker.get_summary()
            result_entry = {
                "id": i + 1,
                "task": args.task,
                "mode": args.mode,
                "input_summary": str(item)[:100].replace("\n", " "),
                "final_answer": ans_str,
                "tokens": run_tokens,
                "cost": current_stats['cost_usd'],
                "time": duration,
                "score": score,
                "eval_details": details
            }
            batch_results.append(result_entry)
            
            # [å³æ™‚å­˜æª”]
            save_batch_results(batch_results, meta_info)

            # [é€²åº¦æ¢ UI]
            progress = (i + 1) / total_items
            filled = int(bar_length * progress)
            bar = 'â–ˆ' * filled + '-' * (bar_length - filled)
            avg_score = total_score / (i + 1)
            
            # ä½¿ç”¨ \r è¦†è“‹ç•¶å‰è¡Œ
            sys.stdout.write(f'\r|{bar}| {i+1}/{total_items} [{(progress*100):.0f}%] - Avg: {avg_score:.2f} - Last: {duration:.1f}s')
            sys.stdout.flush()
            
        except Exception as e:
            # å‡ºéŒ¯æ™‚æ›è¡Œé¡¯ç¤ºï¼Œé¿å…ç ´å£é€²åº¦æ¢
            print(f"\nâš ï¸ Error on item {i+1}: {e}")
            # import traceback; traceback.print_exc()

    print("\n\n" + "="*45)
    final_avg = total_score / total_items if total_items > 0 else 0
    print(f"âœ… Finished {total_items} items.")
    print(f"ğŸ† Final Avg Score: {final_avg:.2%}")
    print(f"ğŸ’° Total Cost: ${tracker.get_summary()['cost_usd']:.6f}")
    print(f"ğŸ“‚ Results saved to: logs/ (Model: {MODEL_NAME})")
    print("="*45)

if __name__ == "__main__":
    main()