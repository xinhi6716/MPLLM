import argparse
import time
import json
import os
import sys
import random  # <--- [æ–°å¢] å¼•å…¥ random
from utils.api_client import get_openai_model_fn
from utils.data_loader import load_dataset
from utils.logger import save_batch_results
from utils.evaluator import evaluate_response
from core.tracker import CostTracker
from pipeline_core import run_mpllm_pipeline

def main():
    # === 0. è¨­å®šé è¨­è·¯å¾‘ ===
    DATA_TRIVIA_PATH = "data/trivia_creative_writing/trivia_creative_writing_100_n_5.jsonl"
    DATA_CODENAMES_PATH = "data/codenames_collaborative/codenames_50.jsonl"
    DATA_LOGIC_PATH = "data/logic_grid_puzzle/logic_grid_puzzle_200.jsonl"

    # 1. è§£æå‘½ä»¤è¡Œåƒæ•¸
    parser = argparse.ArgumentParser(description="MPLLM Nano Runner")
    parser.add_argument('--task', type=str, choices=['trivia', 'codenames', 'logic'], help="Task to run")
    parser.add_argument('--data', type=str, help="Path to .jsonl dataset")
    parser.add_argument('--limit', type=int, default=1, help="Number of items to test")
    parser.add_argument('--interactive', action='store_true', help="Run in interactive chat mode")
    args = parser.parse_args()

    # ==========================================
    # äº’å‹•å¼é¸å–® (ç•¶æ²’æœ‰æŒ‡å®š task æ™‚è§¸ç™¼)
    # ==========================================
    if not args.task and not args.interactive:
        print("\n" + "="*45)
        print(" ğŸ¤– MPLLM Launcher Menu")
        print("="*45)
        
        # --- æ­¥é©Ÿ 1: é¸æ“‡ä»»å‹™ ---
        print("[Step 1] è«‹é¸æ“‡ä»»å‹™:")
        print(" 1. ğŸ“ Trivia (Creative Writing)")
        print(" 2. ğŸ•µï¸  Codenames")
        print(" 3. ğŸ§© Logic Puzzle")
        print("-" * 45)
        print(" 4. ğŸ’¬ è‡ªç”±å°è©±æ¨¡å¼ (Chat Mode - No Scoring)")
        print("="*45)
        
        choice = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é … (1-4): ").strip()
        
        if choice == '1': args.task = 'trivia'
        elif choice == '2': args.task = 'codenames'
        elif choice == '3': args.task = 'logic'
        elif choice == '4': args.interactive = True
        else:
            print("âš ï¸ ç„¡æ•ˆé¸é …ï¼Œé è¨­åŸ·è¡Œ Trivia")
            args.task = 'trivia'

        # --- æ­¥é©Ÿ 2: é¸æ“‡ Single æˆ– Batch (è‹¥éå°è©±æ¨¡å¼) ---
        if not args.interactive:
            print("\n" + "-"*45)
            print(f"[Step 2] é¸æ“‡ '{args.task}' çš„åŸ·è¡Œæ¨¡å¼:")
            print(" 1. ğŸ² Random Single (éš¨æ©ŸæŠ½ 1 é¡Œ)")
            print(" 2. ğŸ“š Sequential Batch (ä¾åºæ¸¬ N é¡Œ)")
            
            mode_choice = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é … (1-2): ").strip()
            
            if mode_choice == '2':
                try:
                    limit_input = input("   è¯·è¾“å…¥è¦åŸ·è¡Œçš„é¡Œæ•¸ (ä¾‹å¦‚ 5, 10): ").strip()
                    args.limit = int(limit_input) if limit_input else 5
                except ValueError:
                    args.limit = 5
                print(f"   ğŸ“š æ¨¡å¼: ä¾åºåŸ·è¡Œå‰ {args.limit} é¡Œ")
            else:
                args.limit = 1
                print("   ğŸ² æ¨¡å¼: éš¨æ©ŸæŠ½å– 1 é¡Œ")

            time.sleep(0.5)

    # === è‡ªå‹•å¡«å…¥è³‡æ–™è·¯å¾‘ ===
    if args.task and not args.data:
        if args.task == 'trivia':
            args.data = DATA_TRIVIA_PATH
        elif args.task == 'codenames':
            args.data = DATA_CODENAMES_PATH
        elif args.task == 'logic':
            args.data = DATA_LOGIC_PATH
        print(f"ğŸ“‚ Auto-selected data: {args.data}")

    # 2. åˆå§‹åŒ–æ¨¡å‹èˆ‡åƒæ•¸
    ARCHITECTURE = "MPLLM"
    MODEL_NAME = "GPT5-Mix"
    TEST_MODE = "Batch" if args.limit > 1 else "Single"

    mini_model = get_openai_model_fn("gpt-5-mini")
    nano_model = get_openai_model_fn("gpt-5-nano")
    models = {'mini': mini_model, 'nano': nano_model}
    tracker = CostTracker()
    batch_results = []

    # === æ¨¡å¼ A: è‡ªç”±å°è©±æ¨¡å¼ ===
    if args.interactive:
        print("\n=== ğŸ’¬ Interactive Chat Mode (No Scoring) ===")
        while True:
            try:
                user_q = input("\nUser Topic: ")
                if user_q.lower() in ['exit', 'quit']: break
                ans, _ = run_mpllm_pipeline('trivia', {"topic": user_q, "questions": []}, models, tracker)
                print(f"\nğŸ¤– MPLLM: {ans}\n")
            except KeyboardInterrupt: break
            except Exception: break
        return

    # === æ¨¡å¼ B: æ•¸æ“šé›†è©•æ¸¬æ¨¡å¼ ===
    print(f"=== ğŸš€ Running: {args.task} | Mode: {TEST_MODE} ===")
    
    dataset = load_dataset(args.task, args.data)
    if not dataset:
        print(f"âŒ Error: Cannot load data from {args.data}")
        return

    # ==========================================
    # [æ ¸å¿ƒä¿®æ”¹] é¡Œç›®é¸æ“‡é‚è¼¯
    # ==========================================
    items_to_process = []
    
    if args.limit == 1:
        # Single Mode: éš¨æ©Ÿé¸ä¸€é¡Œ
        if len(dataset) > 0:
            selected = random.choice(dataset)
            # å˜—è©¦æ‰¾å‡ºå®ƒæ˜¯åŸå§‹è³‡æ–™é›†ä¸­çš„ç¬¬å¹¾é¡Œ (index+1)
            original_idx = dataset.index(selected) + 1
            print(f"ğŸ² Randomly selected Item #{original_idx} (from {len(dataset)} items)")
            items_to_process = [selected]
    else:
        # Batch Mode: é¸å‰ N é¡Œ
        items_to_process = dataset[:args.limit]
        print(f"ğŸ“š Selected top {len(items_to_process)} items sequentially.")

    # ==========================================
    # åŸ·è¡Œè¿´åœˆ
    # ==========================================
    total_score = 0.0
    processed_count = 0
    
    for i, item in enumerate(items_to_process):
        # é¡¯ç¤ºç•¶å‰é€²åº¦ (å¦‚æœæ˜¯éš¨æ©Ÿï¼Œé€™è£¡çš„ i+1 åªæ˜¯åŸ·è¡Œæ¬¡åº)
        print(f"\nğŸ”¸ Processing Task {i+1}/{len(items_to_process)}...")
        start_time = time.time()
        
        try:
            final_ans, trace = run_mpllm_pipeline(args.task, item, models, tracker)
            duration = time.time() - start_time
            
            eval_result = evaluate_response(args.task, final_ans, item)
            score = eval_result.get('score', 0.0)
            details = eval_result.get('details', "")
            
            total_score += score
            processed_count += 1
            
            # é¡¯ç¤ºç°¡åŒ–çµæœ
            ans_str = json.dumps(final_ans, ensure_ascii=False)
            display_str = (ans_str[:75] + '...') if len(ans_str) > 75 else ans_str
            
            print(f"   ğŸ¤– Output: {display_str}") 
            print(f"   ğŸ† Score: {score:.2f} ({details}) | â±ï¸ {duration:.2f}s")
            
            current_stats = tracker.get_summary()
            result_entry = {
                "id": i + 1,
                "task": args.task,
                "input_summary": str(item)[:100].replace("\n", " "),
                "final_answer": ans_str,
                "tokens": trace.get('tokens', 0) if 'tokens' in trace else current_stats.get('total_tokens', 0),
                "cost": current_stats['cost_usd'],
                "time": duration,
                "score": score,
                "eval_details": details
            }
            batch_results.append(result_entry)
            
        except Exception as e:
            print(f"âš ï¸ Error on item: {e}")
            # import traceback; traceback.print_exc()

    if batch_results:
        save_batch_results(batch_results, {
            "architecture": ARCHITECTURE,
            "model": MODEL_NAME,
            "mode": TEST_MODE
        })

    print("\n" + "="*45)
    avg_score = total_score / processed_count if processed_count > 0 else 0
    print(f"âœ… Finished {processed_count} items.")
    print(f"ğŸ† Avg Score: {avg_score:.2%}")
    print(f"ğŸ’° Total Cost: ${tracker.get_summary()['cost_usd']:.6f}")
    print("="*45)

if __name__ == "__main__":
    main()