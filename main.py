# main.py
import os
from utils.api_client import get_openai_model_fn
from utils.logger import log_to_csv
from core.tracker import CostTracker
from pipeline_core import run_mpllm_pipeline

def main():
    # 1. è¨­å®š API Key (è«‹ç¢ºèªç’°å¢ƒè®Šæ•¸æˆ–ç›´æ¥å¡«å…¥)
    # os.environ["OPENAI_API_KEY"] = "sk-..." 
    
    # 2. æº–å‚™ä¾è³´æ³¨å…¥ (Dependency Injection)
    # æˆ‘å€‘å¯ä»¥çµ¦ä¸åŒå±¤ä¸åŒçš„æ¨¡å‹è¨­å®š
    try:
        mini_model = get_openai_model_fn(model_name="gpt-4o-mini")
        # å‡è¨­æˆ‘å€‘æƒ³ç”¨åŒä¸€å€‹æ¨¡å‹æ¨¡æ“¬ nano
        nano_model = get_openai_model_fn(model_name="gpt-4o-mini")
    except ValueError as e:
        print(f"âŒ Error: {e}")
        return

    models = {
        'mini': mini_model,
        'nano': nano_model
    }
    
    tracker = CostTracker()
    
    # 3. æ¸¬è©¦è¼¸å…¥
    user_query = input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ (æˆ–æŒ‰ Enter ä½¿ç”¨é è¨­æ¸¬è©¦é¡Œ): ")
    if not user_query:
        user_query = "è§£é‡‹é‡å­ç³¾çºå¦‚ä½•æ‡‰ç”¨æ–¼æœªä¾†çš„åŠ å¯†æŠ€è¡“ï¼Œä¸¦èˆ‰ä¸€å€‹ç”Ÿæ´»åŒ–çš„ä¾‹å­ã€‚"

    print(f"\nğŸš€ Starting MPLLM for: {user_query}\n" + "="*50)

    # 4. åŸ·è¡Œæµæ°´ç·š
    final_answer, trace_data = run_mpllm_pipeline(user_query, models, tracker)

    # 5. é¡¯ç¤ºçµæœ
    print("="*50)
    print("ğŸ¤– Final Answer:\n")
    print(final_answer)
    print("="*50)
    
    # 6. çµç®—èˆ‡è¨˜éŒ„
    stats = tracker.get_summary()
    print(f"ğŸ’° Cost: ${stats['cost_usd']} | Tokens: {stats['total_tokens']}")
    
    log_data = {
        "input": user_query,
        "final_answer": final_answer,
        "total_tokens": stats['total_tokens'],
        "cost_usd": stats['cost_usd']
    }
    log_to_csv(log_data)

if __name__ == "__main__":
    main()